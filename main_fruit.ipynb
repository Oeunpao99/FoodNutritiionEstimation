{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <span style=\"color: white; font-size: 45px;\">\n",
    "    Image Classification\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations to apply to each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),      # Resize images to 64x64 pixels\n",
    "    transforms.ToTensor(),            # Convert to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 images belonging to 3 classes.\n",
      "Class labels in the training data:\n",
      "{'ahmok': 0, 'apple': 1, 'orange': 2}\n",
      "Epoch 1/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - food_class_accuracy: 0.5257 - loss: 0.8228 - val_food_class_accuracy: 0.8135 - val_loss: 0.5990\n",
      "Epoch 2/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 275ms/step - food_class_accuracy: 0.8497 - loss: 0.4533 - val_food_class_accuracy: 0.8290 - val_loss: 0.3950\n",
      "Epoch 3/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 280ms/step - food_class_accuracy: 0.9304 - loss: 0.2005 - val_food_class_accuracy: 0.9326 - val_loss: 0.3292\n",
      "Epoch 4/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - food_class_accuracy: 0.9478 - loss: 0.1662 - val_food_class_accuracy: 0.9067 - val_loss: 0.3450\n",
      "Epoch 5/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - food_class_accuracy: 0.9520 - loss: 0.1459 - val_food_class_accuracy: 0.9119 - val_loss: 0.3106\n",
      "Epoch 6/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - food_class_accuracy: 0.9575 - loss: 0.1258 - val_food_class_accuracy: 0.9171 - val_loss: 0.3728\n",
      "Epoch 7/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 270ms/step - food_class_accuracy: 0.9616 - loss: 0.1155 - val_food_class_accuracy: 0.9119 - val_loss: 0.3430\n",
      "Epoch 8/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 268ms/step - food_class_accuracy: 0.9633 - loss: 0.1344 - val_food_class_accuracy: 0.9067 - val_loss: 0.3756\n",
      "Epoch 9/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 273ms/step - food_class_accuracy: 0.9689 - loss: 0.0914 - val_food_class_accuracy: 0.9223 - val_loss: 0.3284\n",
      "Epoch 10/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - food_class_accuracy: 0.9593 - loss: 0.1084 - val_food_class_accuracy: 0.8394 - val_loss: 0.6496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the nutritional data from the JSON file\n",
    "with open('D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/nutritional_data.json', 'r') as f:\n",
    "    nutritional_data = json.load(f)\n",
    "\n",
    "# Define image size\n",
    "IMG_SIZE = (150, 150)\n",
    "\n",
    "# Paths to dataset\n",
    "train_dir = 'D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train'\n",
    "test_dir = 'D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/test'\n",
    "\n",
    "# Create ImageDataGenerators for loading images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=30, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True, \n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate data for training and testing\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Print class labels detected by the generator\n",
    "print(\"Class labels in the training data:\")\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "# Define the model architecture with a second output for the quantity\n",
    "def create_model():\n",
    "    input_layer = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "    \n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Flatten the output from the convolutional layers\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Class prediction output\n",
    "    class_output = Dense(len(train_generator.class_indices), activation='softmax', name='food_class')(x)\n",
    "    \n",
    "    # Quantity prediction output (assuming regression for quantity prediction)\n",
    "    quantity_output = Dense(1, activation='linear', name='food_quantity')(x)\n",
    "    \n",
    "    # Create the final model with multiple outputs\n",
    "    model = Model(inputs=input_layer, outputs=[class_output, quantity_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile the model with two loss functions\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), \n",
    "              loss={'food_class': 'categorical_crossentropy', 'food_quantity': 'mse'},\n",
    "              metrics={'food_class': 'accuracy', 'food_quantity': 'mae'})\n",
    "\n",
    "# Train the model (You can add validation data and epochs as needed)\n",
    "model.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('food_recognition_with_quantity_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_url, nutrition_data):\n",
    "    img_array = load_image_from_url(img_url, target_size=(150, 150))\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model('food_recognition_with_quantity_model.h5')\n",
    "    \n",
    "    # Get predictions (food class and quantity)\n",
    "    food_class_pred, food_quantity_pred = model.predict(img_array)\n",
    "    \n",
    "    food_class = np.argmax(food_class_pred, axis=1)\n",
    "    food_name = list(nutrition_data.keys())[food_class[0]]  # Get the food name based on the class\n",
    "    \n",
    "    # Adjust nutrition based on predicted quantity\n",
    "    quantity = food_quantity_pred[0][0]  # Predicted quantity\n",
    "    \n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    if food_nutrition:\n",
    "        adjusted_calories = food_nutrition['calories'] * quantity\n",
    "        adjusted_fat = food_nutrition['fat'] * quantity\n",
    "        adjusted_protein = food_nutrition['protein'] * quantity\n",
    "        adjusted_sugar = food_nutrition['sugar'] * quantity\n",
    "        \n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Predicted Quantity: {quantity}\")\n",
    "        print(f\"Calories: {adjusted_calories} kcal\")\n",
    "        print(f\"Fat: {adjusted_fat} g\")\n",
    "        print(f\"Protein: {adjusted_protein} g\")\n",
    "        print(f\"Sugar: {adjusted_sugar} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple taks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_task_model():\n",
    "    # Input layer for the images\n",
    "    image_input = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "    # Convolutional layers for feature extraction\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "\n",
    "    # Flatten the layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output for food classification (categorical classification)\n",
    "    food_output = Dense(len(nutritional_data), activation='softmax', name='food_classification')(x)\n",
    "\n",
    "    # Output for nutritional predictions (regression for 4 values: calories, fat, protein, sugar)\n",
    "    nutrition_output = Dense(4, activation='linear', name='nutrition_prediction')(x)\n",
    "\n",
    "    # Define the model with two outputs\n",
    "    model = Model(inputs=image_input, outputs=[food_output, nutrition_output])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 images belonging to 3 classes.\n",
      "Found 193 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create an ImageDataGenerator instance for training and testing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Use target_size=(224, 224) to resize images to the required size\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train',  # Path to your training data directory\n",
    "    target_size=(224, 224),  # Resize images to (224, 224)\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # Change as needed depending on your task\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/test',  # Path to your testing data directory\n",
    "    target_size=(224, 224),  # Resize images to (224, 224)\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # Change as needed depending on your task\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()  # Enable eager execution if not already enabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 527ms/step - food_classification_accuracy: 0.9850 - loss: 0.0428 - val_food_classification_accuracy: 0.9223 - val_loss: 0.4822\n",
      "Epoch 2/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 493ms/step - food_classification_accuracy: 0.9945 - loss: 0.0265 - val_food_classification_accuracy: 0.9326 - val_loss: 0.5292\n",
      "Epoch 3/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 490ms/step - food_classification_accuracy: 0.9947 - loss: 0.0273 - val_food_classification_accuracy: 0.9171 - val_loss: 0.5954\n",
      "Epoch 4/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 486ms/step - food_classification_accuracy: 0.9988 - loss: 0.0127 - val_food_classification_accuracy: 0.9223 - val_loss: 0.5787\n",
      "Epoch 5/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 487ms/step - food_classification_accuracy: 0.9969 - loss: 0.0165 - val_food_classification_accuracy: 0.9171 - val_loss: 0.5715\n",
      "Epoch 6/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 482ms/step - food_classification_accuracy: 1.0000 - loss: 0.0119 - val_food_classification_accuracy: 0.9171 - val_loss: 0.6250\n",
      "Epoch 7/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 482ms/step - food_classification_accuracy: 0.9946 - loss: 0.0155 - val_food_classification_accuracy: 0.9223 - val_loss: 0.6724\n",
      "Epoch 8/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 486ms/step - food_classification_accuracy: 0.9940 - loss: 0.0196 - val_food_classification_accuracy: 0.9378 - val_loss: 0.5747\n",
      "Epoch 9/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 485ms/step - food_classification_accuracy: 0.9964 - loss: 0.0116 - val_food_classification_accuracy: 0.9275 - val_loss: 0.5764\n",
      "Epoch 10/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 501ms/step - food_classification_accuracy: 0.9989 - loss: 0.0083 - val_food_classification_accuracy: 0.9275 - val_loss: 0.5761\n",
      "Epoch 11/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 486ms/step - food_classification_accuracy: 0.9916 - loss: 0.0193 - val_food_classification_accuracy: 0.9223 - val_loss: 0.6896\n",
      "Epoch 12/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 480ms/step - food_classification_accuracy: 0.9978 - loss: 0.0120 - val_food_classification_accuracy: 0.9223 - val_loss: 0.5493\n",
      "Epoch 13/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 480ms/step - food_classification_accuracy: 0.9918 - loss: 0.0145 - val_food_classification_accuracy: 0.9223 - val_loss: 0.5653\n",
      "Epoch 14/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 485ms/step - food_classification_accuracy: 0.9987 - loss: 0.0068 - val_food_classification_accuracy: 0.9275 - val_loss: 0.5852\n",
      "Epoch 15/15\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 479ms/step - food_classification_accuracy: 0.9993 - loss: 0.0096 - val_food_classification_accuracy: 0.9223 - val_loss: 0.5608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Training the model for 15 epochs\n",
    "epochs = 15\n",
    "\n",
    "# Ensure the model is compiled correctly before fitting\n",
    "history = model.fit(train_generator,  # Training data generator (ensure this is defined correctly)\n",
    "    validation_data=test_generator,  # Validation data generator (ensure this is defined correctly)\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('food_recognition_with_nutrition_models.h5')\n",
    "\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image_from_url\u001b[39m(img_url, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Fetch the image from the URL\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(img_url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_image_from_url(img_url, target_size=(224, 224)):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    # Resize the image to match the model input size\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array and normalize it\n",
    "    img_array = np.array(img) / 255.0  # Normalize if necessary\n",
    "    \n",
    "    # Add batch dimension (model expects 4D input: batch_size, height, width, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def load_nutrition_info(json_file='D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/nutritional_data.json'):\n",
    "    with open(json_file, 'r') as file:\n",
    "        nutrition_data = json.load(file)\n",
    "    return nutrition_data\n",
    "\n",
    "def predict_image(img_url, nutrition_data, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Load the model (ensure you have the trained model saved as .h5)\n",
    "    model = tf.keras.models.load_model('food_recognition_with_nutrition_models.h5')\n",
    "    \n",
    "    # Get the predictions for both outputs (food classification and nutrition prediction)\n",
    "    food_classification_pred, nutrition_prediction_pred = model.predict(img_array)\n",
    "    \n",
    "    # Process the results\n",
    "    food_class = np.argmax(food_classification_pred, axis=1)  # Get the class with the highest probability\n",
    "    food_name = list(class_labels.keys())[food_class[0]]  # Get the food name based on the class index\n",
    "    \n",
    "    # Get the predicted nutrition value\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    if food_nutrition:\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Calories: {food_nutrition['calories']} kcal\")\n",
    "        print(f\"Fat: {food_nutrition['fat']} g\")\n",
    "        print(f\"Protein: {food_nutrition['protein']} g\")\n",
    "        print(f\"Sugar: {food_nutrition['sugar']} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n",
    "\n",
    "# Example image URL (replace with an actual image URL you want to test)\n",
    "img_url = 'https://thumbs.dreamstime.com/b/orage-isolated-white-background-d-style-305530202.jpg'\n",
    "\n",
    "# Load the nutrition data from the JSON file\n",
    "nutrition_data = load_nutrition_info()\n",
    "\n",
    "# Define the class labels for mapping prediction to food names\n",
    "class_labels = {'ahmok': 0, 'apple': 1, 'orange': 2}\n",
    "\n",
    "# Make predictions for the image\n",
    "predict_image(img_url, nutrition_data, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "Predicted Food: Apple\n",
      "Predicted Food Class: 1 (Index)\n",
      "Number of Food Items: 3\n",
      "Total Calories: 285 kcal\n",
      "Total Fat: 0.8999999999999999 g\n",
      "Total Protein: 1.5 g\n",
      "Total Sugar: 57 g\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_image_from_url(img_url, target_size=(224, 224)):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    # Resize the image to match the model input size\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array and normalize it\n",
    "    img_array = np.array(img) / 255.0  # Normalize if necessary\n",
    "    \n",
    "    # Add batch dimension (model expects 4D input: batch_size, height, width, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def load_nutrition_info(json_file='D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/nutritional_data.json'):\n",
    "    with open(json_file, 'r') as file:\n",
    "        nutrition_data = json.load(file)\n",
    "    return nutrition_data\n",
    "\n",
    "def predict_image(img_url, nutrition_data, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Load the model (ensure you have the trained model saved as .h5)\n",
    "    model = tf.keras.models.load_model('food_recognition_with_nutrition_models.h5')\n",
    "    \n",
    "    # Get the predictions for both outputs (food classification and nutrition prediction)\n",
    "    food_classification_pred, nutrition_prediction_pred = model.predict(img_array)\n",
    "    \n",
    "    # Process the food classification results\n",
    "    food_class = np.argmax(food_classification_pred, axis=1)  # Get the class with the highest probability\n",
    "    food_name = list(class_labels.keys())[food_class[0]]  # Get the food name based on the class index\n",
    "    \n",
    "    # Get the predicted nutrition value\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    # Number of food items (For now, assuming one fruit per image; you can integrate object detection for actual counting)\n",
    "    number_of_food = 3  # Change this if you want to detect multiple items, e.g., with an object detection model\n",
    "    \n",
    "    # Calculate total nutrition for the detected number of food items\n",
    "    if food_nutrition:\n",
    "        total_calories = food_nutrition['calories'] * number_of_food\n",
    "        total_fat = food_nutrition['fat'] * number_of_food\n",
    "        total_protein = food_nutrition['protein'] * number_of_food\n",
    "        total_sugar = food_nutrition['sugar'] * number_of_food\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Predicted Food Class: {food_class[0]} (Index)\")\n",
    "        print(f\"Number of Food Items: {number_of_food}\")\n",
    "        print(f\"Total Calories: {total_calories} kcal\")\n",
    "        print(f\"Total Fat: {total_fat} g\")\n",
    "        print(f\"Total Protein: {total_protein} g\")\n",
    "        print(f\"Total Sugar: {total_sugar} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n",
    "\n",
    "# Example image URL (replace with an actual image URL you want to test)\n",
    "img_url = 'https://5.imimg.com/data5/AK/RA/MY-68428614/apple.jpg'\n",
    "\n",
    "# Load the nutrition data from the JSON file\n",
    "nutrition_data = load_nutrition_info()\n",
    "\n",
    "# Define the class labels for mapping prediction to food names\n",
    "class_labels = {'ahmok': 0, 'apple': 1, 'orange': 2}  # Update this with your actual labels\n",
    "\n",
    "# Make predictions for the image\n",
    "predict_image(img_url, nutrition_data, class_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parth2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def build_model(num_classes=3):  # Adjust based on your actual number of classes\n",
    "    # Load a pre-trained ResNet50 model (without the top classification layer)\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze the base model layers to retain pre-trained weights\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom layers on top for both tasks\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Task 1: Classification head (food classification)\n",
    "    class_output = layers.Dense(num_classes, activation='softmax', name='class_output')(x)\n",
    "    \n",
    "    # Task 2: Quantity head (predict number of food items)\n",
    "    quantity_output = layers.Dense(1, activation='linear', name='quantity_output')(x)\n",
    "\n",
    "    # Create the final model\n",
    "    model = models.Model(inputs=base_model.input, outputs=[class_output, quantity_output])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'class_output': 'categorical_crossentropy', 'quantity_output': 'mse'},\n",
    "                  metrics={'class_output': 'accuracy', 'quantity_output': 'mae'})\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_image_from_url(img_url, target_size=(224, 224)):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    # Resize the image to match the model input size\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array and normalize it\n",
    "    img_array = np.array(img) / 255.0  # Normalize if necessary\n",
    "    \n",
    "    # Add batch dimension (model expects 4D input: batch_size, height, width, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def predict_image(img_url, model, class_labels, nutrition_data):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Make predictions\n",
    "    class_pred, quantity_pred = model.predict(img_array)\n",
    "    \n",
    "    # Get the predicted class (food type)\n",
    "    food_class = np.argmax(class_pred, axis=1)\n",
    "    food_name = list(class_labels.keys())[food_class[0]]\n",
    "    \n",
    "    # Get the predicted quantity (how many items)\n",
    "    number_of_food = round(quantity_pred[0][0])\n",
    "    \n",
    "    # Get the nutrition info for the predicted food\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    # Calculate total nutrition\n",
    "    if food_nutrition:\n",
    "        total_calories = food_nutrition['calories'] * number_of_food\n",
    "        total_fat = food_nutrition['fat'] * number_of_food\n",
    "        total_protein = food_nutrition['protein'] * number_of_food\n",
    "        total_sugar = food_nutrition['sugar'] * number_of_food\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Predicted Food Class: {food_class[0]} (Index)\")\n",
    "        print(f\"Number of Food Items: {number_of_food}\")\n",
    "        print(f\"Total Calories: {total_calories} kcal\")\n",
    "        print(f\"Total Fat: {total_fat} g\")\n",
    "        print(f\"Total Protein: {total_protein} g\")\n",
    "        print(f\"Total Sugar: {total_sugar} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def preprocess_and_augment_data(image_dir, batch_size=32, target_size=(224, 224)):\n",
    "    # Initialize ImageDataGenerator for data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Flow images from the directory for training (make sure your data is organized in subdirectories by class)\n",
    "    train_data = datagen.flow_from_directory(\n",
    "        image_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - class_output_accuracy: 0.2423 - loss: 1.9905\n",
      "Epoch 2/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - class_output_accuracy: 0.4418 - loss: 0.7671\n",
      "Epoch 3/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - class_output_accuracy: 0.5088 - loss: 0.7183\n",
      "Epoch 4/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - class_output_accuracy: 0.6046 - loss: 0.7595\n",
      "Epoch 5/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - class_output_accuracy: 0.5126 - loss: 0.7455\n",
      "Epoch 6/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - class_output_accuracy: 0.5334 - loss: 0.7170\n",
      "Epoch 7/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - class_output_accuracy: 0.6009 - loss: 0.7287\n",
      "Epoch 8/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - class_output_accuracy: 0.5573 - loss: 0.7096\n",
      "Epoch 9/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - class_output_accuracy: 0.5670 - loss: 0.7078\n",
      "Epoch 10/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - class_output_accuracy: 0.6451 - loss: 0.6831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Load your data for training\n",
    "train_data = preprocess_and_augment_data(image_dir='D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train', batch_size=32)\n",
    "\n",
    "# Build the model\n",
    "model = build_model(num_classes=len(class_labels))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, epochs=10, steps_per_epoch=len(train_data) // 32)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('food_recognition_with_nutrition_modell.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "\n",
    "# Load the trained model with custom metrics\n",
    "def load_trained_model(model_path='food_recognition_with_nutrition_modell.h5'):\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'mse': MeanSquaredError()})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "\n",
    "# Load the trained model (with custom objects, if needed)\n",
    "def load_trained_model(model_path='food_recognition_with_nutrition_modell.h5'):\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'MeanSquaredError': MeanSquaredError()})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Food: Apple\n",
      "Predicted Food Class: 1 (Index)\n",
      "Number of Food Items: 1\n",
      "Total Calories: 95 kcal\n",
      "Total Fat: 0.3 g\n",
      "Total Protein: 0.5 g\n",
      "Total Sugar: 19 g\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import json  # Make sure to import json for loading the nutrition data\n",
    "\n",
    "# Load the nutrition data (replace this with the actual path to your nutrition data file)\n",
    "def load_nutrition_info(json_file='nutritional_data.json'):\n",
    "    with open(json_file, 'r') as file:\n",
    "        nutrition_data = json.load(file)\n",
    "    return nutrition_data\n",
    "\n",
    "# Load an image from URL and preprocess it\n",
    "def load_image_from_url(img_url, target_size=(224, 224)):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    # Resize the image to match the model input size\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array and normalize it\n",
    "    img_array = np.array(img) / 255.0  # Normalize if necessary\n",
    "    \n",
    "    # Add batch dimension (model expects 4D input: batch_size, height, width, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# Load the trained model (ensure you have the trained model saved)\n",
    "def load_trained_model(model_path='food_recognition_with_nutrition_models.h5'):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "# Predict the food type, quantity, and total nutrition from an image\n",
    "def predict_food_and_quantity(img_url, model, class_labels, nutrition_data):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Make predictions (food classification and quantity prediction)\n",
    "    food_class_pred, quantity_pred = model.predict(img_array)\n",
    "    \n",
    "    # Get the predicted food class (food type)\n",
    "    food_class = np.argmax(food_class_pred, axis=1)\n",
    "    food_name = list(class_labels.keys())[food_class[0]]  # Map index to food name\n",
    "    \n",
    "    # Get the predicted quantity (number of food items)\n",
    "    # If quantity prediction is continuous, round it to the nearest integer\n",
    "    number_of_food = max(1, round(quantity_pred[0][0]))  # Ensure quantity is at least 1\n",
    "    \n",
    "    # Get the nutrition info for the predicted food\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    # Calculate total nutrition for the predicted quantity\n",
    "    if food_nutrition:\n",
    "        total_calories = food_nutrition['calories'] * number_of_food\n",
    "        total_fat = food_nutrition['fat'] * number_of_food\n",
    "        total_protein = food_nutrition['protein'] * number_of_food\n",
    "        total_sugar = food_nutrition['sugar'] * number_of_food\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Predicted Food Class: {food_class[0]} (Index)\")\n",
    "        print(f\"Number of Food Items: {number_of_food}\")\n",
    "        print(f\"Total Calories: {total_calories} kcal\")\n",
    "        print(f\"Total Fat: {total_fat} g\")\n",
    "        print(f\"Total Protein: {total_protein} g\")\n",
    "        print(f\"Total Sugar: {total_sugar} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n",
    "\n",
    "# Example image URL (replace with an actual image URL you want to test)\n",
    "img_url = 'https://assets.clevelandclinic.org/transform/cd71f4bd-81d4-45d8-a450-74df78e4477a/Apples-184940975-770x533-1_jpg'\n",
    "\n",
    "# Example class labels (ensure you have the correct mapping)\n",
    "class_labels = {'ahmok': 0, 'apple': 1, 'orange': 2}  # Update this based on your actual classes\n",
    "\n",
    "# Load the nutrition data (adjust path to your file)\n",
    "nutrition_data = load_nutrition_info('nutritional_data.json')\n",
    "\n",
    "# Load the trained model\n",
    "model = load_trained_model('food_recognition_with_nutrition_models.h5')\n",
    "\n",
    "# Predict food type, quantity, and total nutrition\n",
    "predict_food_and_quantity(img_url, model, class_labels, nutrition_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "def create_train_generator(directory, batch_size=32, target_size=(224, 224)):\n",
    "    # Initialize the ImageDataGenerator with rescaling\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Get class labels and quantity information from the directory structure\n",
    "    def quantity_label_from_dir_name(dir_name):\n",
    "        # You need to define how to extract the quantity (number of food items)\n",
    "        # For example, if the folder name is \"apple_5\", extract the quantity '5'\n",
    "        return int(dir_name.split('_')[-1])  # Assuming the quantity is at the end of the folder name\n",
    "\n",
    "    # Create a custom generator to yield both image data and (class, quantity) labels\n",
    "    def custom_train_generator():\n",
    "        for batch_x, batch_y in train_datagen.flow_from_directory(\n",
    "            directory,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical'  # Only food class\n",
    "        ):\n",
    "            # Get the directory names for the labels\n",
    "            labels = batch_y\n",
    "            quantity_labels = [quantity_label_from_dir_name(name) for name in os.listdir(directory)]\n",
    "            \n",
    "            # Yield images, food class labels, and quantity labels\n",
    "            yield batch_x, {'food_class': labels, 'quantity': quantity_labels}\n",
    "\n",
    "    return custom_train_generator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def quantity_label_from_dir_name(dir_name):\n",
    "    # Use regex to search for a number at the end of the directory name\n",
    "    match = re.search(r'(\\d+)$', dir_name)  # This will match any digits at the end of the string\n",
    "    \n",
    "    if match:\n",
    "        return int(match.group(1))  # Extract and return the number as an integer\n",
    "    else:\n",
    "        # If no number is found, return a default value or handle the case accordingly\n",
    "        print(f\"Warning: No quantity found in directory name: {dir_name}\")\n",
    "        return 1  # Default value, adjust as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutrition data for apple: {'calories': 95, 'fat': 0.3, 'protein': 0.5, 'sugar': 19}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the nutritional data from the JSON file\n",
    "def load_nutrition_info(json_file='nutritional_data.json'):\n",
    "    with open(json_file, 'r') as f:\n",
    "        nutrition_data = json.load(f)\n",
    "    return nutrition_data\n",
    "\n",
    "# Extract nutrition for a food item\n",
    "def get_food_nutrition(food_name, nutrition_data):\n",
    "    # Use the food name to fetch nutritional information\n",
    "    food_data = nutrition_data.get(food_name)\n",
    "    if food_data:\n",
    "        return food_data\n",
    "    else:\n",
    "        print(f\"Warning: Nutrition data for {food_name} not found!\")\n",
    "        return None  # Return None or a default value if the food is not found\n",
    "\n",
    "# Example usage\n",
    "nutrition_data = load_nutrition_info('nutritional_data.json')\n",
    "food_name = 'apple'  # Example food\n",
    "nutrition_info = get_food_nutrition(food_name, nutrition_data)\n",
    "\n",
    "if nutrition_info:\n",
    "    print(f\"Nutrition data for {food_name}: {nutrition_info}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 images belonging to 3 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'ahmok'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m create_train_generator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust the steps per epoch based on your dataset size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[107], line 24\u001b[0m, in \u001b[0;36mcreate_train_generator.<locals>.custom_train_generator\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     17\u001b[0m     directory,\n\u001b[0;32m     18\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m ):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Get the directory names for the labels\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch_y\n\u001b[1;32m---> 24\u001b[0m     quantity_labels \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mquantity_label_from_dir_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Yield images, food class labels, and quantity labels\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch_x, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfood_class\u001b[39m\u001b[38;5;124m'\u001b[39m: labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m'\u001b[39m: quantity_labels}\n",
      "Cell \u001b[1;32mIn[107], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     17\u001b[0m     directory,\n\u001b[0;32m     18\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m ):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Get the directory names for the labels\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch_y\n\u001b[1;32m---> 24\u001b[0m     quantity_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mquantity_label_from_dir_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory)]\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Yield images, food class labels, and quantity labels\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch_x, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfood_class\u001b[39m\u001b[38;5;124m'\u001b[39m: labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m'\u001b[39m: quantity_labels}\n",
      "Cell \u001b[1;32mIn[107], line 12\u001b[0m, in \u001b[0;36mcreate_train_generator.<locals>.quantity_label_from_dir_name\u001b[1;34m(dir_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantity_label_from_dir_name\u001b[39m(dir_name):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# You need to define how to extract the quantity (number of food items)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# For example, if the folder name is \"apple_5\", extract the quantity '5'\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'ahmok'"
     ]
    }
   ],
   "source": [
    "# Set the number of food classes (e.g., 3 classes: apple, orange, banana)\n",
    "num_classes = 3\n",
    "\n",
    "# Build the model\n",
    "model = build_model(num_classes=num_classes)\n",
    "\n",
    "# Create the training generator\n",
    "train_generator = create_train_generator('D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, steps_per_epoch=100)  # Adjust the steps per epoch based on your dataset size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ADMIN/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-2-12 Python-3.11.5 torch-2.2.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutrition data not available for carrot.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the YOLOv5 model (using PyTorch)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Use 'yolov5m' or 'yolov5l' for larger models\n",
    "\n",
    "# Load image from URL or local path\n",
    "img_url = 'https://grantourismotravels.com/wp-content/uploads/2017/05/Authentic-Fish-Amok-Recipe-Steamed-Fish-Curry-Cambodia-Copyright-2022-Terence-Carter-Grantourismo-T-500x375.jpg'\n",
    "img = Image.open(requests.get(img_url, stream=True).raw)\n",
    "\n",
    "# Run inference (detect objects in the image)\n",
    "results = model(img)\n",
    "\n",
    "# Results contains detected objects with labels, confidence, and bounding boxes\n",
    "predicted_labels = results.names  # The class names\n",
    "predicted_boxes = results.xywh[0]  # Predicted bounding boxes (x, y, width, height)\n",
    "confidences = results.xywh[0][:, 4]  # Confidence scores\n",
    "classes = results.xywh[0][:, 5].tolist()  # Class indices\n",
    "\n",
    "# Count instances of each class (food items)\n",
    "food_counts = {}\n",
    "for cls in classes:\n",
    "    food_name = predicted_labels[int(cls)]\n",
    "    food_counts[food_name] = food_counts.get(food_name, 0) + 1\n",
    "\n",
    "# Nutrition data for the food items (example data structure)\n",
    "nutrition_data = {\n",
    "    'apple': {'calories': 100, 'fat': 0.5, 'protein': 0.5, 'sugar': 19},\n",
    "    'orange': {'calories': 80, 'fat': 0.3, 'protein': 1.5, 'sugar': 18},\n",
    "    'grape': {'calories': 80, 'fat': 0.3, 'protein': 1.5, 'sugar': 18}\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# Calculate total nutrition based on the detected quantity\n",
    "for food_name, count in food_counts.items():\n",
    "    if food_name in nutrition_data:\n",
    "        food_nutrition = nutrition_data[food_name]\n",
    "        total_calories = food_nutrition['calories'] * count\n",
    "        total_fat = food_nutrition['fat'] * count\n",
    "        total_protein = food_nutrition['protein'] * count\n",
    "        total_sugar = food_nutrition['sugar'] * count\n",
    "\n",
    "        # Display results\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Number of Food Items: {count}\")\n",
    "        print(f\"Total Calories: {total_calories} kcal\")\n",
    "        print(f\"Total Fat: {total_fat} g\")\n",
    "        print(f\"Total Protein: {total_protein} g\")\n",
    "        print(f\"Total Sugar: {total_sugar} g\")\n",
    "    else:\n",
    "        print(f\"Nutrition data not available for {food_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ADMIN/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-2-12 Python-3.11.5 torch-2.2.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutrition data not available for sandwich.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch  # We need torch to load YOLOv5\n",
    "import json  # To load the nutrition data\n",
    "\n",
    "# Load the nutrition data (replace this with the actual path to your nutrition data file)\n",
    "def load_nutrition_info(json_file='nutritional_data.json'):\n",
    "    with open(json_file, 'r') as file:\n",
    "        nutrition_data = json.load(file)\n",
    "    return nutrition_data\n",
    "\n",
    "# Load an image from URL and preprocess it\n",
    "def load_image_from_url(img_url):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Load YOLOv5 model (use a custom-trained YOLOv5 model if you have one)\n",
    "def load_yolov5_model(model_path='yolov5s.pt'):  # yolov5s is a small, pre-trained model\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)  # Load custom model\n",
    "    return model\n",
    "\n",
    "# Predict the food type, quantity, and total nutrition from an image using YOLOv5\n",
    "def predict_food_and_quantity(img_url, model, nutrition_data, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img = load_image_from_url(img_url)\n",
    "    \n",
    "    # Perform inference with YOLOv5\n",
    "    results = model(img)  # Inference\n",
    "    \n",
    "    # Get the results from YOLO (bounding boxes, labels, and confidence scores)\n",
    "    detected_classes = results.names  # Class names predicted by YOLO\n",
    "    detected_boxes = results.xywh[0]  # Bounding boxes (x, y, width, height)\n",
    "    confidences = results.xywh[0][:, 4]  # Confidence scores for each detection\n",
    "    class_indices = results.xywh[0][:, 5].tolist()  # Class indices for each detected object\n",
    "    \n",
    "    # Count and calculate nutrition for each detected food item\n",
    "    food_count = {}\n",
    "    \n",
    "    for idx, cls_idx in enumerate(class_indices):\n",
    "        food_name = detected_classes[cls_idx]\n",
    "        \n",
    "        # Increase the count for each detected food type\n",
    "        if food_name in food_count:\n",
    "            food_count[food_name] += 1\n",
    "        else:\n",
    "            food_count[food_name] = 1\n",
    "    \n",
    "    # Calculate nutrition for each food detected\n",
    "    for food_name, count in food_count.items():\n",
    "        food_nutrition = nutrition_data.get(food_name, None)\n",
    "        \n",
    "        if food_nutrition:\n",
    "            total_calories = food_nutrition['calories'] * count\n",
    "            total_fat = food_nutrition['fat'] * count\n",
    "            total_protein = food_nutrition['protein'] * count\n",
    "            total_sugar = food_nutrition['sugar'] * count\n",
    "            \n",
    "            # Display the results\n",
    "            print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "            print(f\"Count: {count}\")\n",
    "            print(f\"Total Calories: {total_calories} kcal\")\n",
    "            print(f\"Total Fat: {total_fat} g\")\n",
    "            print(f\"Total Protein: {total_protein} g\")\n",
    "            print(f\"Total Sugar: {total_sugar} g\")\n",
    "        else:\n",
    "            print(f\"Nutrition data not available for {food_name}.\")\n",
    "\n",
    "# Example image URL (replace with an actual image URL you want to test)\n",
    "img_url = 'https://ca-times.brightspotcdn.com/dims4/default/b243748/2147483647/strip/true/crop/8100x5400+0+0/resize/2000x1333!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F1e%2Fde%2F897ca4cd4327a0e7d2328bae202c%2F1419161-fo-sophys-review-rad-003.jpg'\n",
    "\n",
    "# Example class labels (ensure you have the correct mapping for your food classes)\n",
    "class_labels = {'ahmok': 0, 'apple': 1, 'orange': 2}  # Update this based on your actual classes\n",
    "\n",
    "# Load the nutrition data (adjust path to your file)\n",
    "nutrition_data = load_nutrition_info('nutritional_data.json')\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = load_yolov5_model('yolov5s.pt')  # Replace with your custom-trained model if available\n",
    "\n",
    "# Predict food type, quantity, and total nutrition\n",
    "predict_food_and_quantity(img_url, model, nutrition_data, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('food_recognition_with_nutrition_models.h5')\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Function to preprocess the image and make predictions\n",
    "def process_image(image: UploadFile):\n",
    "    # Read image\n",
    "    img_bytes = image.file.read()\n",
    "    img = Image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "    # Preprocess the image as required by your model\n",
    "    img = img.resize((224, 224))  # Resize to the expected input size for the model\n",
    "    img_array = np.array(img) / 255.0  # Normalize the image\n",
    "\n",
    "    # Add batch dimension to image array\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    # Assuming the model returns nutrition values directly\n",
    "    nutrition_data = {\n",
    "        \"calories\": prediction[0][0],\n",
    "        \"protein\": prediction[0][1],\n",
    "        \"fat\": prediction[0][2],\n",
    "        \"sugar\": prediction[0][3],\n",
    "    }\n",
    "    return nutrition_data\n",
    "\n",
    "@app.post(\"/upload/\")\n",
    "async def upload_image(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        nutrition_data = process_image(file)\n",
    "        return JSONResponse(content=nutrition_data)\n",
    "    except Exception as e:\n",
    "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpb7iix0k5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpb7iix0k5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpb7iix0k5'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_6')\n",
      "Output Type:\n",
      "  List[TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)]\n",
      "Captures:\n",
      "  1609766711568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1609766710224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1609766711760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1609766710992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1609766710800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1609766704272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1609766710032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1609766709264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load your Keras model\n",
    "model = tf.keras.models.load_model('C:/xampp/htdocs/php-practics/fnapp/model/food_recognition_with_nutrition_models.h5')\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model\n",
    "with open('C:/xampp/htdocs/php-practics/fnapp/model/fn.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
