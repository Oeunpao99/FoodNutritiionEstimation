{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <span style=\"color: white; font-size: 45px;\">\n",
    "    Image Classification\n",
    "  </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations to apply to each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),      # Resize images to 64x64 pixels\n",
    "    transforms.ToTensor(),            # Convert to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 images belonging to 3 classes.\n",
      "Class labels in the training data:\n",
      "{'ahmok': 0, 'apple': 1, 'orange': 2}\n",
      "Epoch 1/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - food_class_accuracy: 0.5257 - loss: 0.8228 - val_food_class_accuracy: 0.8135 - val_loss: 0.5990\n",
      "Epoch 2/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 275ms/step - food_class_accuracy: 0.8497 - loss: 0.4533 - val_food_class_accuracy: 0.8290 - val_loss: 0.3950\n",
      "Epoch 3/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 280ms/step - food_class_accuracy: 0.9304 - loss: 0.2005 - val_food_class_accuracy: 0.9326 - val_loss: 0.3292\n",
      "Epoch 4/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - food_class_accuracy: 0.9478 - loss: 0.1662 - val_food_class_accuracy: 0.9067 - val_loss: 0.3450\n",
      "Epoch 5/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - food_class_accuracy: 0.9520 - loss: 0.1459 - val_food_class_accuracy: 0.9119 - val_loss: 0.3106\n",
      "Epoch 6/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - food_class_accuracy: 0.9575 - loss: 0.1258 - val_food_class_accuracy: 0.9171 - val_loss: 0.3728\n",
      "Epoch 7/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 270ms/step - food_class_accuracy: 0.9616 - loss: 0.1155 - val_food_class_accuracy: 0.9119 - val_loss: 0.3430\n",
      "Epoch 8/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 268ms/step - food_class_accuracy: 0.9633 - loss: 0.1344 - val_food_class_accuracy: 0.9067 - val_loss: 0.3756\n",
      "Epoch 9/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 273ms/step - food_class_accuracy: 0.9689 - loss: 0.0914 - val_food_class_accuracy: 0.9223 - val_loss: 0.3284\n",
      "Epoch 10/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - food_class_accuracy: 0.9593 - loss: 0.1084 - val_food_class_accuracy: 0.8394 - val_loss: 0.6496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the nutritional data from the JSON file\n",
    "with open('D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/nutritional_data.json', 'r') as f:\n",
    "    nutritional_data = json.load(f)\n",
    "\n",
    "# Define image size\n",
    "IMG_SIZE = (150, 150)\n",
    "\n",
    "# Paths to dataset\n",
    "train_dir = 'D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train'\n",
    "test_dir = 'D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/test'\n",
    "\n",
    "# Create ImageDataGenerators for loading images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=30, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True, \n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate data for training and testing\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Print class labels detected by the generator\n",
    "print(\"Class labels in the training data:\")\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "# Define the model architecture with a second output for the quantity\n",
    "def create_model():\n",
    "    input_layer = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "    \n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Flatten the output from the convolutional layers\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Class prediction output\n",
    "    class_output = Dense(len(train_generator.class_indices), activation='softmax', name='food_class')(x)\n",
    "    \n",
    "    # Quantity prediction output (assuming regression for quantity prediction)\n",
    "    quantity_output = Dense(1, activation='linear', name='food_quantity')(x)\n",
    "    \n",
    "    # Create the final model with multiple outputs\n",
    "    model = Model(inputs=input_layer, outputs=[class_output, quantity_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile the model with two loss functions\n",
    "model = create_model()\n",
    "model.compile(optimizer=Adam(), \n",
    "              loss={'food_class': 'categorical_crossentropy', 'food_quantity': 'mse'},\n",
    "              metrics={'food_class': 'accuracy', 'food_quantity': 'mae'})\n",
    "\n",
    "# Train the model (You can add validation data and epochs as needed)\n",
    "model.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('food_recognition_with_quantity_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_url, nutrition_data):\n",
    "    img_array = load_image_from_url(img_url, target_size=(150, 150))\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model('food_recognition_with_quantity_model.h5')\n",
    "    \n",
    "    # Get predictions (food class and quantity)\n",
    "    food_class_pred, food_quantity_pred = model.predict(img_array)\n",
    "    \n",
    "    food_class = np.argmax(food_class_pred, axis=1)\n",
    "    food_name = list(nutrition_data.keys())[food_class[0]]  # Get the food name based on the class\n",
    "    \n",
    "    # Adjust nutrition based on predicted quantity\n",
    "    quantity = food_quantity_pred[0][0]  # Predicted quantity\n",
    "    \n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    if food_nutrition:\n",
    "        adjusted_calories = food_nutrition['calories'] * quantity\n",
    "        adjusted_fat = food_nutrition['fat'] * quantity\n",
    "        adjusted_protein = food_nutrition['protein'] * quantity\n",
    "        adjusted_sugar = food_nutrition['sugar'] * quantity\n",
    "        \n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Predicted Quantity: {quantity}\")\n",
    "        print(f\"Calories: {adjusted_calories} kcal\")\n",
    "        print(f\"Fat: {adjusted_fat} g\")\n",
    "        print(f\"Protein: {adjusted_protein} g\")\n",
    "        print(f\"Sugar: {adjusted_sugar} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple taks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_task_model():\n",
    "    # Input layer for the images\n",
    "    image_input = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "    # Convolutional layers for feature extraction\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "\n",
    "    # Flatten the layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output for food classification (categorical classification)\n",
    "    food_output = Dense(len(nutritional_data), activation='softmax', name='food_classification')(x)\n",
    "\n",
    "    # Output for nutritional predictions (regression for 4 values: calories, fat, protein, sugar)\n",
    "    nutrition_output = Dense(4, activation='linear', name='nutrition_prediction')(x)\n",
    "\n",
    "    # Define the model with two outputs\n",
    "    model = Model(inputs=image_input, outputs=[food_output, nutrition_output])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 614 images belonging to 12 classes.\n",
      "Found 194 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Use target_size=(224, 224) to resize images to the required size\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train',  # Path to your training data directory\n",
    "    target_size=(224, 224),  \n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  \n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/test',  # Path to your testing data directory\n",
    "    target_size=(224, 224),  # Resize images to (224, 224)\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # Change as needed depending on your task\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 563 images belonging to 12 classes.\n",
      "Found 606 images belonging to 12 classes.\n",
      "Epoch 1/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.2414 - loss: 2.6958 - val_accuracy: 0.4472 - val_loss: 1.5818\n",
      "Epoch 2/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.4835 - loss: 1.6220 - val_accuracy: 0.6997 - val_loss: 1.1808\n",
      "Epoch 3/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.5973 - loss: 1.3188 - val_accuracy: 0.7277 - val_loss: 0.9818\n",
      "Epoch 4/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.7116 - loss: 1.0357 - val_accuracy: 0.7442 - val_loss: 0.8402\n",
      "Epoch 5/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - accuracy: 0.7514 - loss: 0.8583 - val_accuracy: 0.7624 - val_loss: 0.7427\n",
      "Epoch 6/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.7560 - loss: 0.7781 - val_accuracy: 0.7937 - val_loss: 0.6805\n",
      "Epoch 7/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.8149 - loss: 0.6278 - val_accuracy: 0.8119 - val_loss: 0.6305\n",
      "Epoch 8/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.7657 - loss: 0.7270 - val_accuracy: 0.8284 - val_loss: 0.5889\n",
      "Epoch 9/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.8298 - loss: 0.5617 - val_accuracy: 0.8432 - val_loss: 0.5578\n",
      "Epoch 10/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.8512 - loss: 0.4718 - val_accuracy: 0.8581 - val_loss: 0.5289\n",
      "Epoch 11/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.8024 - loss: 0.5622 - val_accuracy: 0.8729 - val_loss: 0.5022\n",
      "Epoch 12/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.8220 - loss: 0.5587 - val_accuracy: 0.8795 - val_loss: 0.4805\n",
      "Epoch 13/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.8597 - loss: 0.4261 - val_accuracy: 0.8845 - val_loss: 0.4629\n",
      "Epoch 14/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.8811 - loss: 0.3947 - val_accuracy: 0.8993 - val_loss: 0.4446\n",
      "Epoch 15/15\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.8808 - loss: 0.4082 - val_accuracy: 0.9059 - val_loss: 0.4210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load Pretrained Model (MobileNetV2)\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers (keep pretrained weights)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "num_classes = len(train_generator.class_indices)  # Get number of classes\n",
    "output_layer = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=30, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\", target_size=(224, 224), batch_size=32, class_mode=\"categorical\")\n",
    "test_generator = test_datagen.flow_from_directory(\"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/test\", target_size=(224, 224), batch_size=32, class_mode=\"categorical\")\n",
    "\n",
    "# Train the model\n",
    "epochs = 15\n",
    "\n",
    "history = model.fit(train_generator,  \n",
    "    validation_data=test_generator,  \n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('food_recognition_nutrition_model.h5')\n",
    "\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_27856\\2062534845.py:2: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()  # Enable eager execution if not already enabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import json\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"food_recognition_nutrition_model.h5\")\n",
    "\n",
    "# Load class labels (from train_generator)\n",
    "class_labels = {\n",
    "    0: 'Amok',\n",
    "    1: 'Apple',\n",
    "    2: 'BorBor',\n",
    "    3: 'Cha Loc Lac',\n",
    "    4: 'Chicken Rice',\n",
    "    5: 'Lemongrass BBQ Beef Skewers',\n",
    "    6: 'Lort Cha',\n",
    "    7: 'Nom Banh Chok',\n",
    "    8: 'Omelette Rice with Pork',\n",
    "    9: 'Orange Fruit',\n",
    "    10: 'Red Curry Coconut Wings',\n",
    "    11: 'Samlar KorKour'\n",
    "}\n",
    "\n",
    "# Load nutrition data from JSON file\n",
    "def load_nutrition_info(json_file=\"nutritional_data.json\"):\n",
    "    with open(json_file, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "nutrition_data = load_nutrition_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and predict the image\n",
    "def predict_food(image_path):\n",
    "    # Load image and preprocess\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img) / 255.0  # Normalize\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]  # Get the highest probability class index\n",
    "    food_name = class_labels[predicted_class]\n",
    "\n",
    "    # Get nutrition info\n",
    "    nutrition_info = nutrition_data.get(food_name, {\"calories\": \"N/A\", \"fat\": \"N/A\", \"protein\": \"N/A\", \"sugar\": \"N/A\"})\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Predicted Food: {food_name}\")\n",
    "    print(f\"Calories: {nutrition_info['calories']} kcal\")\n",
    "    print(f\"Fat: {nutrition_info['fat']} g\")\n",
    "    print(f\"Protein: {nutrition_info['protein']} g\")\n",
    "    print(f\"Sugar: {nutrition_info['sugar']} g\")\n",
    "\n",
    "    return food_name, nutrition_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000015A18E29620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000015A18E29620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842ms/step\n",
      "Predicted Food: Lemongrass BBQ Beef Skewers\n",
      "Calories: 611.65 kcal\n",
      "Carbs: 13.43 g\n",
      "Fat: 3.5 g\n",
      "Protein: 88.49 g\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('food_recognition_nutrition_model.h5')\n",
    "\n",
    "# Load class labels (same order as training)\n",
    "class_labels = [\n",
    "    \"Amok\", \"apple\", \"BorBor\", \"Cha Loc Lac\", \"Chikhen Rice\", \"Lemongrass BBQ Beef Skewers\", \n",
    "    \"Lort Cha\", \"Nom Banh Chok\", \"Omelete Rice with Pork\", \"Orage Fruit\", \n",
    "    \"Red Curry Coconut Wings\", \"Samlar KorKour\"\n",
    "]\n",
    "\n",
    "# Load Nutrition Data\n",
    "def load_nutrition_info(json_file=\"nutritional_data.json\"):\n",
    "    with open(json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Convert list to dictionary for easy lookup\n",
    "    return {item[\"name\"]: {\"calories\": item[\"calories\"], \"carbs\": item[\"carbs\"], \"fat\": item[\"fat\"], \"protein\": item[\"protein\"]} for item in data}\n",
    "\n",
    "nutrition_data = load_nutrition_info()\n",
    "\n",
    "# Function to preprocess and predict food from an image\n",
    "def predict_food(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Normalize\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)  # Get index of highest probability class\n",
    "    food_name = class_labels[predicted_class]\n",
    "\n",
    "    # Get nutrition info\n",
    "    nutrition_info = nutrition_data.get(food_name, {\"calories\": \"N/A\", \"carbs\": \"N/A\", \"fat\": \"N/A\", \"protein\": \"N/A\"})\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Predicted Food: {food_name}\")\n",
    "    print(f\"Calories: {nutrition_info['calories']} kcal\")\n",
    "    print(f\"Carbs: {nutrition_info['carbs']} g\")\n",
    "    print(f\"Fat: {nutrition_info['fat']} g\")\n",
    "    print(f\"Protein: {nutrition_info['protein']} g\")\n",
    "\n",
    "# Example Test\n",
    "image_path = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/test/Nom Banh Chok/6168441767_2032207878_b.jpg\"  # Change this to your test image path\n",
    "\n",
    "predict_food(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/test/Amok/Cambodian-Khmer-Fish-Amok-Recipe-13.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change this to your test image path\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mpredict_food\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mpredict_food\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m food_name \u001b[38;5;241m=\u001b[39m class_labels[predicted_class]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get nutrition info\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m nutrition_info \u001b[38;5;241m=\u001b[39m \u001b[43mnutrition_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(food_name, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalories\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msugar\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Food: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfood_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Provide the path of an image to test\n",
    "\n",
    "# Predict\n",
    "predict_food(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_url, nutrition_data, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Load the model (ensure you have the trained model saved as .h5)\n",
    "    model = tf.keras.models.load_model('food_recognition_nutrition_model.h5')\n",
    "    \n",
    "    # Get the food classification prediction (if model has one output)\n",
    "    food_classification_pred = model.predict(img_array)\n",
    "    \n",
    "    # Process the results\n",
    "    food_class = np.argmax(food_classification_pred, axis=1)  # Get the class with the highest probability\n",
    "    food_name = list(class_labels.keys())[food_class[0]]  # Get the food name based on the class index\n",
    "    \n",
    "    # Get the predicted nutrition value (adjust this according to your model structure)\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    if food_nutrition:\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Calories: {food_nutrition['calories']} kcal\")\n",
    "        print(f\"Fat: {food_nutrition['fat']} g\")\n",
    "        print(f\"Protein: {food_nutrition['protein']} g\")\n",
    "        print(f\"Sugar: {food_nutrition['sugar']} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_url, nutrition_data, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Load the model (ensure you have the trained model saved as .h5)\n",
    "    model = tf.keras.models.load_model('food_recognition_nutrition_model.h5')\n",
    "    \n",
    "    # Get the food classification prediction (if model has one output)\n",
    "    food_classification_pred = model.predict(img_array)\n",
    "    \n",
    "    # Process the results\n",
    "    food_class = np.argmax(food_classification_pred, axis=1)  # Get the class with the highest probability\n",
    "    food_name = list(class_labels.keys())[food_class[0]]  # Get the food name based on the class index\n",
    "    \n",
    "    # Get the predicted nutrition value (adjust this according to your model structure)\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    if food_nutrition:\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Calories: {food_nutrition['calories']} kcal\")\n",
    "        print(f\"Fat: {food_nutrition['fat']} g\")\n",
    "        print(f\"Protein: {food_nutrition['protein']} g\")\n",
    "        print(f\"Sugar: {food_nutrition['sugar']} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mahmok\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapple\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}  \u001b[38;5;66;03m# Update this with your actual labels\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Make predictions for the image\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnutrition_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 43\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(img_url, nutrition_data, class_labels)\u001b[0m\n\u001b[0;32m     40\u001b[0m food_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(class_labels\u001b[38;5;241m.\u001b[39mkeys())[food_class[\u001b[38;5;241m0\u001b[39m]]  \u001b[38;5;66;03m# Get the food name based on the class index\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Get the predicted nutrition value\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m food_nutrition \u001b[38;5;241m=\u001b[39m \u001b[43mnutrition_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(food_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Number of food items (For now, assuming one fruit per image; you can integrate object detection for actual counting)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m number_of_food \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Change this if you want to detect multiple items, e.g., with an object detection model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_image_from_url(img_url, target_size=(224, 224)):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    # Resize the image to match the model input size\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array and normalize it\n",
    "    img_array = np.array(img) / 255.0  # Normalize if necessary\n",
    "    \n",
    "    # Add batch dimension (model expects 4D input: batch_size, height, width, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def load_nutrition_info(json_file='D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/nutritional_data.json'):\n",
    "    with open(json_file, 'r') as file:\n",
    "        nutrition_data = json.load(file)\n",
    "    return nutrition_data\n",
    "\n",
    "def predict_image(img_url, nutrition_data, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Load the model (ensure you have the trained model saved as .h5)\n",
    "    model = tf.keras.models.load_model('food_recognition_with_nutrition_models.h5')\n",
    "    \n",
    "    # Get the predictions for both outputs (food classification and nutrition prediction)\n",
    "    food_classification_pred, nutrition_prediction_pred = model.predict(img_array)\n",
    "    \n",
    "    # Process the food classification results\n",
    "    food_class = np.argmax(food_classification_pred, axis=1)  # Get the class with the highest probability\n",
    "    food_name = list(class_labels.keys())[food_class[0]]  # Get the food name based on the class index\n",
    "    \n",
    "    # Get the predicted nutrition value\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    # Number of food items (For now, assuming one fruit per image; you can integrate object detection for actual counting)\n",
    "    number_of_food = 3  # Change this if you want to detect multiple items, e.g., with an object detection model\n",
    "    \n",
    "    # Calculate total nutrition for the detected number of food items\n",
    "    if food_nutrition:\n",
    "        total_calories = food_nutrition['calories'] * number_of_food\n",
    "        total_fat = food_nutrition['fat'] * number_of_food\n",
    "        total_protein = food_nutrition['protein'] * number_of_food\n",
    "        total_sugar = food_nutrition['sugar'] * number_of_food\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Predicted Food Class: {food_class[0]} (Index)\")\n",
    "        print(f\"Number of Food Items: {number_of_food}\")\n",
    "        print(f\"Total Calories: {total_calories} kcal\")\n",
    "        print(f\"Total Fat: {total_fat} g\")\n",
    "        print(f\"Total Protein: {total_protein} g\")\n",
    "        print(f\"Total Sugar: {total_sugar} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n",
    "\n",
    "# Example image URL (replace with an actual image URL you want to test)\n",
    "img_url = 'https://5.imimg.com/data5/AK/RA/MY-68428614/apple.jpg'\n",
    "\n",
    "# Load the nutrition data from the JSON file\n",
    "nutrition_data = load_nutrition_info()\n",
    "\n",
    "# Define the class labels for mapping prediction to food names\n",
    "class_labels = {'ahmok': 0, 'apple': 1, 'orange': 2}  # Update this with your actual labels\n",
    "\n",
    "# Make predictions for the image\n",
    "predict_image(img_url, nutrition_data, class_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parth2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def build_model(num_classes=3):  # Adjust based on your actual number of classes\n",
    "    # Load a pre-trained ResNet50 model (without the top classification layer)\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze the base model layers to retain pre-trained weights\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom layers on top for both tasks\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Task 1: Classification head (food classification)\n",
    "    class_output = layers.Dense(num_classes, activation='softmax', name='class_output')(x)\n",
    "    \n",
    "    # Task 2: Quantity head (predict number of food items)\n",
    "    quantity_output = layers.Dense(1, activation='linear', name='quantity_output')(x)\n",
    "\n",
    "    # Create the final model\n",
    "    model = models.Model(inputs=base_model.input, outputs=[class_output, quantity_output])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'class_output': 'categorical_crossentropy', 'quantity_output': 'mse'},\n",
    "                  metrics={'class_output': 'accuracy', 'quantity_output': 'mae'})\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_image_from_url(img_url, target_size=(224, 224)):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    # Resize the image to match the model input size\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array and normalize it\n",
    "    img_array = np.array(img) / 255.0  # Normalize if necessary\n",
    "    \n",
    "    # Add batch dimension (model expects 4D input: batch_size, height, width, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "def predict_image(img_url, model, class_labels, nutrition_data):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Make predictions\n",
    "    class_pred, quantity_pred = model.predict(img_array)\n",
    "    \n",
    "    # Get the predicted class (food type)\n",
    "    food_class = np.argmax(class_pred, axis=1)\n",
    "    food_name = list(class_labels.keys())[food_class[0]]\n",
    "    \n",
    "    # Get the predicted quantity (how many items)\n",
    "    number_of_food = round(quantity_pred[0][0])\n",
    "    \n",
    "    # Get the nutrition info for the predicted food\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    # Calculate total nutrition\n",
    "    if food_nutrition:\n",
    "        total_calories = food_nutrition['calories'] * number_of_food\n",
    "        total_fat = food_nutrition['fat'] * number_of_food\n",
    "        total_protein = food_nutrition['protein'] * number_of_food\n",
    "        total_sugar = food_nutrition['sugar'] * number_of_food\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Predicted Food Class: {food_class[0]} (Index)\")\n",
    "        print(f\"Number of Food Items: {number_of_food}\")\n",
    "        print(f\"Total Calories: {total_calories} kcal\")\n",
    "        print(f\"Total Fat: {total_fat} g\")\n",
    "        print(f\"Total Protein: {total_protein} g\")\n",
    "        print(f\"Total Sugar: {total_sugar} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def preprocess_and_augment_data(image_dir, batch_size=32, target_size=(224, 224)):\n",
    "    # Initialize ImageDataGenerator for data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Flow images from the directory for training (make sure your data is organized in subdirectories by class)\n",
    "    train_data = datagen.flow_from_directory(\n",
    "        image_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - class_output_accuracy: 0.2423 - loss: 1.9905\n",
      "Epoch 2/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - class_output_accuracy: 0.4418 - loss: 0.7671\n",
      "Epoch 3/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - class_output_accuracy: 0.5088 - loss: 0.7183\n",
      "Epoch 4/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - class_output_accuracy: 0.6046 - loss: 0.7595\n",
      "Epoch 5/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - class_output_accuracy: 0.5126 - loss: 0.7455\n",
      "Epoch 6/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - class_output_accuracy: 0.5334 - loss: 0.7170\n",
      "Epoch 7/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - class_output_accuracy: 0.6009 - loss: 0.7287\n",
      "Epoch 8/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - class_output_accuracy: 0.5573 - loss: 0.7096\n",
      "Epoch 9/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - class_output_accuracy: 0.5670 - loss: 0.7078\n",
      "Epoch 10/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - class_output_accuracy: 0.6451 - loss: 0.6831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Load your data for training\n",
    "train_data = preprocess_and_augment_data(image_dir='D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train', batch_size=32)\n",
    "\n",
    "# Build the model\n",
    "model = build_model(num_classes=len(class_labels))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, epochs=10, steps_per_epoch=len(train_data) // 32)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('food_recognition_with_nutrition_modell.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "\n",
    "# Load the trained model with custom metrics\n",
    "def load_trained_model(model_path='food_recognition_with_nutrition_modell.h5'):\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'mse': MeanSquaredError()})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "\n",
    "# Load the trained model (with custom objects, if needed)\n",
    "def load_trained_model(model_path='food_recognition_with_nutrition_modell.h5'):\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'MeanSquaredError': MeanSquaredError()})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Food: Apple\n",
      "Predicted Food Class: 1 (Index)\n",
      "Number of Food Items: 1\n",
      "Total Calories: 95 kcal\n",
      "Total Fat: 0.3 g\n",
      "Total Protein: 0.5 g\n",
      "Total Sugar: 19 g\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import json  # Make sure to import json for loading the nutrition data\n",
    "\n",
    "# Load the nutrition data (replace this with the actual path to your nutrition data file)\n",
    "def load_nutrition_info(json_file='nutritional_data.json'):\n",
    "    with open(json_file, 'r') as file:\n",
    "        nutrition_data = json.load(file)\n",
    "    return nutrition_data\n",
    "\n",
    "# Load an image from URL and preprocess it\n",
    "def load_image_from_url(img_url, target_size=(224, 224)):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    # Resize the image to match the model input size\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array and normalize it\n",
    "    img_array = np.array(img) / 255.0  # Normalize if necessary\n",
    "    \n",
    "    # Add batch dimension (model expects 4D input: batch_size, height, width, channels)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# Load the trained model (ensure you have the trained model saved)\n",
    "def load_trained_model(model_path='food_recognition_with_nutrition_models.h5'):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "# Predict the food type, quantity, and total nutrition from an image\n",
    "def predict_food_and_quantity(img_url, model, class_labels, nutrition_data):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_image_from_url(img_url, target_size=(224, 224))\n",
    "    \n",
    "    # Make predictions (food classification and quantity prediction)\n",
    "    food_class_pred, quantity_pred = model.predict(img_array)\n",
    "    \n",
    "    # Get the predicted food class (food type)\n",
    "    food_class = np.argmax(food_class_pred, axis=1)\n",
    "    food_name = list(class_labels.keys())[food_class[0]]  # Map index to food name\n",
    "    \n",
    "    # Get the predicted quantity (number of food items)\n",
    "    # If quantity prediction is continuous, round it to the nearest integer\n",
    "    number_of_food = max(1, round(quantity_pred[0][0]))  # Ensure quantity is at least 1\n",
    "    \n",
    "    # Get the nutrition info for the predicted food\n",
    "    food_nutrition = nutrition_data.get(food_name, None)\n",
    "    \n",
    "    # Calculate total nutrition for the predicted quantity\n",
    "    if food_nutrition:\n",
    "        total_calories = food_nutrition['calories'] * number_of_food\n",
    "        total_fat = food_nutrition['fat'] * number_of_food\n",
    "        total_protein = food_nutrition['protein'] * number_of_food\n",
    "        total_sugar = food_nutrition['sugar'] * number_of_food\n",
    "        \n",
    "        # Display the results\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Predicted Food Class: {food_class[0]} (Index)\")\n",
    "        print(f\"Number of Food Items: {number_of_food}\")\n",
    "        print(f\"Total Calories: {total_calories} kcal\")\n",
    "        print(f\"Total Fat: {total_fat} g\")\n",
    "        print(f\"Total Protein: {total_protein} g\")\n",
    "        print(f\"Total Sugar: {total_sugar} g\")\n",
    "    else:\n",
    "        print(\"Nutrition data not available for this food.\")\n",
    "\n",
    "# Example image URL (replace with an actual image URL you want to test)\n",
    "img_url = 'https://assets.clevelandclinic.org/transform/cd71f4bd-81d4-45d8-a450-74df78e4477a/Apples-184940975-770x533-1_jpg'\n",
    "\n",
    "# Example class labels (ensure you have the correct mapping)\n",
    "class_labels = {'ahmok': 0, 'apple': 1, 'orange': 2}  # Update this based on your actual classes\n",
    "\n",
    "# Load the nutrition data (adjust path to your file)\n",
    "nutrition_data = load_nutrition_info('nutritional_data.json')\n",
    "\n",
    "# Load the trained model\n",
    "model = load_trained_model('food_recognition_with_nutrition_models.h5')\n",
    "\n",
    "# Predict food type, quantity, and total nutrition\n",
    "predict_food_and_quantity(img_url, model, class_labels, nutrition_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "def create_train_generator(directory, batch_size=32, target_size=(224, 224)):\n",
    "    # Initialize the ImageDataGenerator with rescaling\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Get class labels and quantity information from the directory structure\n",
    "    def quantity_label_from_dir_name(dir_name):\n",
    "        # You need to define how to extract the quantity (number of food items)\n",
    "        # For example, if the folder name is \"apple_5\", extract the quantity '5'\n",
    "        return int(dir_name.split('_')[-1])  # Assuming the quantity is at the end of the folder name\n",
    "\n",
    "    # Create a custom generator to yield both image data and (class, quantity) labels\n",
    "    def custom_train_generator():\n",
    "        for batch_x, batch_y in train_datagen.flow_from_directory(\n",
    "            directory,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical'  # Only food class\n",
    "        ):\n",
    "            # Get the directory names for the labels\n",
    "            labels = batch_y\n",
    "            quantity_labels = [quantity_label_from_dir_name(name) for name in os.listdir(directory)]\n",
    "            \n",
    "            # Yield images, food class labels, and quantity labels\n",
    "            yield batch_x, {'food_class': labels, 'quantity': quantity_labels}\n",
    "\n",
    "    return custom_train_generator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def quantity_label_from_dir_name(dir_name):\n",
    "    # Use regex to search for a number at the end of the directory name\n",
    "    match = re.search(r'(\\d+)$', dir_name)  # This will match any digits at the end of the string\n",
    "    \n",
    "    if match:\n",
    "        return int(match.group(1))  # Extract and return the number as an integer\n",
    "    else:\n",
    "        # If no number is found, return a default value or handle the case accordingly\n",
    "        print(f\"Warning: No quantity found in directory name: {dir_name}\")\n",
    "        return 1  # Default value, adjust as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutrition data for apple: {'calories': 95, 'fat': 0.3, 'protein': 0.5, 'sugar': 19}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the nutritional data from the JSON file\n",
    "def load_nutrition_info(json_file='nutritional_data.json'):\n",
    "    with open(json_file, 'r') as f:\n",
    "        nutrition_data = json.load(f)\n",
    "    return nutrition_data\n",
    "\n",
    "# Extract nutrition for a food item\n",
    "def get_food_nutrition(food_name, nutrition_data):\n",
    "    # Use the food name to fetch nutritional information\n",
    "    food_data = nutrition_data.get(food_name)\n",
    "    if food_data:\n",
    "        return food_data\n",
    "    else:\n",
    "        print(f\"Warning: Nutrition data for {food_name} not found!\")\n",
    "        return None  # Return None or a default value if the food is not found\n",
    "\n",
    "# Example usage\n",
    "nutrition_data = load_nutrition_info('nutritional_data.json')\n",
    "food_name = 'apple'  # Example food\n",
    "nutrition_info = get_food_nutrition(food_name, nutrition_data)\n",
    "\n",
    "if nutrition_info:\n",
    "    print(f\"Nutrition data for {food_name}: {nutrition_info}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 465 images belonging to 3 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'ahmok'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m create_train_generator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust the steps per epoch based on your dataset size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[107], line 24\u001b[0m, in \u001b[0;36mcreate_train_generator.<locals>.custom_train_generator\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     17\u001b[0m     directory,\n\u001b[0;32m     18\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m ):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Get the directory names for the labels\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch_y\n\u001b[1;32m---> 24\u001b[0m     quantity_labels \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mquantity_label_from_dir_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Yield images, food class labels, and quantity labels\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch_x, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfood_class\u001b[39m\u001b[38;5;124m'\u001b[39m: labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m'\u001b[39m: quantity_labels}\n",
      "Cell \u001b[1;32mIn[107], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     17\u001b[0m     directory,\n\u001b[0;32m     18\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m ):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Get the directory names for the labels\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch_y\n\u001b[1;32m---> 24\u001b[0m     quantity_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mquantity_label_from_dir_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory)]\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Yield images, food class labels, and quantity labels\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch_x, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfood_class\u001b[39m\u001b[38;5;124m'\u001b[39m: labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m'\u001b[39m: quantity_labels}\n",
      "Cell \u001b[1;32mIn[107], line 12\u001b[0m, in \u001b[0;36mcreate_train_generator.<locals>.quantity_label_from_dir_name\u001b[1;34m(dir_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantity_label_from_dir_name\u001b[39m(dir_name):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# You need to define how to extract the quantity (number of food items)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# For example, if the folder name is \"apple_5\", extract the quantity '5'\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'ahmok'"
     ]
    }
   ],
   "source": [
    "# Set the number of food classes (e.g., 3 classes: apple, orange, banana)\n",
    "num_classes = 3\n",
    "\n",
    "# Build the model\n",
    "model = build_model(num_classes=num_classes)\n",
    "\n",
    "# Create the training generator\n",
    "train_generator = create_train_generator('D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, steps_per_epoch=100)  # Adjust the steps per epoch based on your dataset size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ADMIN/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-2-12 Python-3.11.5 torch-2.2.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutrition data not available for carrot.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the YOLOv5 model (using PyTorch)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Use 'yolov5m' or 'yolov5l' for larger models\n",
    "\n",
    "# Load image from URL or local path\n",
    "img_url = 'https://grantourismotravels.com/wp-content/uploads/2017/05/Authentic-Fish-Amok-Recipe-Steamed-Fish-Curry-Cambodia-Copyright-2022-Terence-Carter-Grantourismo-T-500x375.jpg'\n",
    "img = Image.open(requests.get(img_url, stream=True).raw)\n",
    "\n",
    "# Run inference (detect objects in the image)\n",
    "results = model(img)\n",
    "\n",
    "# Results contains detected objects with labels, confidence, and bounding boxes\n",
    "predicted_labels = results.names  # The class names\n",
    "predicted_boxes = results.xywh[0]  # Predicted bounding boxes (x, y, width, height)\n",
    "confidences = results.xywh[0][:, 4]  # Confidence scores\n",
    "classes = results.xywh[0][:, 5].tolist()  # Class indices\n",
    "\n",
    "# Count instances of each class (food items)\n",
    "food_counts = {}\n",
    "for cls in classes:\n",
    "    food_name = predicted_labels[int(cls)]\n",
    "    food_counts[food_name] = food_counts.get(food_name, 0) + 1\n",
    "\n",
    "# Nutrition data for the food items (example data structure)\n",
    "nutrition_data = {\n",
    "    'apple': {'calories': 100, 'fat': 0.5, 'protein': 0.5, 'sugar': 19},\n",
    "    'orange': {'calories': 80, 'fat': 0.3, 'protein': 1.5, 'sugar': 18},\n",
    "    'grape': {'calories': 80, 'fat': 0.3, 'protein': 1.5, 'sugar': 18}\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# Calculate total nutrition based on the detected quantity\n",
    "for food_name, count in food_counts.items():\n",
    "    if food_name in nutrition_data:\n",
    "        food_nutrition = nutrition_data[food_name]\n",
    "        total_calories = food_nutrition['calories'] * count\n",
    "        total_fat = food_nutrition['fat'] * count\n",
    "        total_protein = food_nutrition['protein'] * count\n",
    "        total_sugar = food_nutrition['sugar'] * count\n",
    "\n",
    "        # Display results\n",
    "        print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "        print(f\"Number of Food Items: {count}\")\n",
    "        print(f\"Total Calories: {total_calories} kcal\")\n",
    "        print(f\"Total Fat: {total_fat} g\")\n",
    "        print(f\"Total Protein: {total_protein} g\")\n",
    "        print(f\"Total Sugar: {total_sugar} g\")\n",
    "    else:\n",
    "        print(f\"Nutrition data not available for {food_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ADMIN/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-2-12 Python-3.11.5 torch-2.2.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Food: Apple\n",
      "Count: 1\n",
      "Total Calories: 95 kcal\n",
      "Total Fat: 0.3 g\n",
      "Total Protein: 0.5 g\n",
      "Total Sugar: 19 g\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch  # We need torch to load YOLOv5\n",
    "import json  # To load the nutrition data\n",
    "\n",
    "# Load the nutrition data (replace this with the actual path to your nutrition data file)\n",
    "def load_nutrition_info(json_file='nutritional_data.json'):\n",
    "    with open(json_file, 'r') as file:\n",
    "        nutrition_data = json.load(file)\n",
    "    return nutrition_data\n",
    "\n",
    "# Load an image from URL and preprocess it\n",
    "def load_image_from_url(img_url):\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(img_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Load YOLOv5 model (use a custom-trained YOLOv5 model if you have one)\n",
    "def load_yolov5_model(model_path='yolov5s.pt'):  # yolov5s is a small, pre-trained model\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)  # Load custom model\n",
    "    return model\n",
    "\n",
    "# Predict the food type, quantity, and total nutrition from an image using YOLOv5\n",
    "def predict_food_and_quantity(img_url, model, nutrition_data, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img = load_image_from_url(img_url)\n",
    "    \n",
    "    # Perform inference with YOLOv5\n",
    "    results = model(img)  # Inference\n",
    "    \n",
    "    # Get the results from YOLO (bounding boxes, labels, and confidence scores)\n",
    "    detected_classes = results.names  # Class names predicted by YOLO\n",
    "    detected_boxes = results.xywh[0]  # Bounding boxes (x, y, width, height)\n",
    "    confidences = results.xywh[0][:, 4]  # Confidence scores for each detection\n",
    "    class_indices = results.xywh[0][:, 5].tolist()  # Class indices for each detected object\n",
    "    \n",
    "    # Count and calculate nutrition for each detected food item\n",
    "    food_count = {}\n",
    "    \n",
    "    for idx, cls_idx in enumerate(class_indices):\n",
    "        food_name = detected_classes[cls_idx]\n",
    "        \n",
    "        # Increase the count for each detected food type\n",
    "        if food_name in food_count:\n",
    "            food_count[food_name] += 1\n",
    "        else:\n",
    "            food_count[food_name] = 1\n",
    "    \n",
    "    # Calculate nutrition for each food detected\n",
    "    for food_name, count in food_count.items():\n",
    "        food_nutrition = nutrition_data.get(food_name, None)\n",
    "        \n",
    "        if food_nutrition:\n",
    "            total_calories = food_nutrition['calories'] * count\n",
    "            total_fat = food_nutrition['fat'] * count\n",
    "            total_protein = food_nutrition['protein'] * count\n",
    "            total_sugar = food_nutrition['sugar'] * count\n",
    "            \n",
    "            # Display the results\n",
    "            print(f\"Predicted Food: {food_name.capitalize()}\")\n",
    "            print(f\"Count: {count}\")\n",
    "            print(f\"Total Calories: {total_calories} kcal\")\n",
    "            print(f\"Total Fat: {total_fat} g\")\n",
    "            print(f\"Total Protein: {total_protein} g\")\n",
    "            print(f\"Total Sugar: {total_sugar} g\")\n",
    "        else:\n",
    "            print(f\"Nutrition data not available for {food_name}.\")\n",
    "\n",
    "\n",
    "img_url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcREEj1k_FQp0Tk0PhjSciVzjpRs4QibES_TRw&s'\n",
    "\n",
    "# Example class labels (ensure you have the correct mapping for your food classes)\n",
    "class_labels = {'ahmok': 0, 'apple': 1, 'orange': 2}  # Update this based on your actual classes\n",
    "\n",
    "\n",
    "nutrition_data = load_nutrition_info('nutritional_data.json')\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = load_yolov5_model('yolov5s.pt')  # Replace with your custom-trained model if available\n",
    "\n",
    "predict_food_and_quantity(img_url, model, nutrition_data, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation completed! 5 new images per original image.\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),  # Flip horizontally with 50% chance\n",
    "    A.RandomBrightnessContrast(p=0.5),  # Adjust brightness and contrast\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.7),  # Random shift, scale, rotate\n",
    "    A.GaussianBlur(p=0.3),  # Apply Gaussian blur randomly\n",
    "    # A.RandomSizedCrop(min_max_height=(120, 150), height=150, width=150, p=0.5),  # Random crop & resize\n",
    "    A.RandomShadow(p=0.3),  # Simulate lighting conditions\n",
    "    A.RandomRain(p=0.2),  # Simulate rainy conditions\n",
    "])\n",
    "\n",
    "# Paths\n",
    "input_folder = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train/chikhen_rice\"\n",
    "output_folder = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/augmented/chikhen_rice\"\n",
    "\n",
    "# Create output folder if not exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Number of augmented images per original image\n",
    "num_augmentations = 5  # Change this to control the number of variations\n",
    "\n",
    "# Process each image in the input folder\n",
    "for img_name in os.listdir(input_folder):\n",
    "    img_path = os.path.join(input_folder, img_name)\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        print(f\"Skipping {img_name}, unable to read.\")\n",
    "        continue\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    for i in range(num_augmentations):  # Create multiple versions\n",
    "        # Apply augmentation\n",
    "        augmented = transform(image=image)[\"image\"]\n",
    "\n",
    "        # Convert back to OpenCV format (denormalize & scale)\n",
    "        augmented = np.clip((augmented * 255), 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Save augmented image\n",
    "        output_path = os.path.join(output_folder, f\"aug_{i}_{img_name}\")\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "print(f\"Data augmentation completed! {num_augmentations} new images per original image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdct\u001b[49m)  \u001b[38;5;66;03m# Check if it contains 'InitSchema' and required fields\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dct' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Using a pre-trained model (e.g., ResNet)\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(512, 256)  # Output feature vector\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output1 = self.forward_once(x1)\n",
    "        output2 = self.forward_once(x2)\n",
    "        return F.pairwise_distance(output1, output2)  # Distance between images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, distance, label):\n",
    "        loss = (1-label) * torch.pow(distance, 2) + label * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\ADMIN/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:13<00:00, 3.53MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 17.2795\n",
      "Epoch 2, Loss: 0.4586\n",
      "Epoch 3, Loss: 1.2489\n",
      "Epoch 4, Loss: 0.7574\n",
      "Epoch 5, Loss: 0.4755\n",
      "Epoch 6, Loss: 0.2640\n",
      "Epoch 7, Loss: 0.3181\n",
      "Epoch 8, Loss: 0.4685\n",
      "Epoch 9, Loss: 0.2666\n",
      "Epoch 10, Loss: 0.3163\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Custom Dataset for One-Shot Learning\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(image_folder)\n",
    "        self.image_paths = {c: [os.path.join(image_folder, c, img) for img in os.listdir(os.path.join(image_folder, c))] for c in self.classes}\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(imgs) for imgs in self.image_paths.values())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        class1 = random.choice(self.classes)\n",
    "        image1_path = random.choice(self.image_paths[class1])\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            class2 = class1  # Same class (positive pair)\n",
    "        else:\n",
    "            class2 = random.choice(self.classes)  # Different class (negative pair)\n",
    "\n",
    "        image2_path = random.choice(self.image_paths[class2])\n",
    "\n",
    "        image1 = Image.open(image1_path).convert(\"RGB\")\n",
    "        image2 = Image.open(image2_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "\n",
    "        label = 1 if class1 == class2 else 0  # 1 for same class, 0 for different\n",
    "        return image1, image2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# Define DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = FoodDataset(\"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for img1, img2, label in dataloader:\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img1, img2)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Food Class: ahmok\n"
     ]
    }
   ],
   "source": [
    "def predict(model, image_path, reference_folder):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.Resize((150, 150)), transforms.ToTensor()])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    best_match = None\n",
    "\n",
    "    for class_name in os.listdir(reference_folder):\n",
    "        for ref_image_path in os.listdir(os.path.join(reference_folder, class_name)):\n",
    "            ref_image = Image.open(os.path.join(reference_folder, class_name, ref_image_path)).convert(\"RGB\")\n",
    "            ref_image = transform(ref_image).unsqueeze(0).to(device)\n",
    "            distance = model(image, ref_image).item()\n",
    "\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_match = class_name\n",
    "\n",
    "    return best_match\n",
    "\n",
    "# Predict food class\n",
    "food_class = predict(model, \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/apple\\img_21.jpeg\", \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\")\n",
    "print(f\"Predicted Food Class: {food_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Amok: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Amok_augmented\n",
      "✅ apple: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\apple_augmented\n",
      "✅ Borbor: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Borbor_augmented\n",
      "✅ Cha Loc Lac: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Cha Loc Lac_augmented\n",
      "✅ Chikhen Rice: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Chikhen Rice_augmented\n",
      "✅ Lemongrass BBQ Beef Skewers: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Lemongrass BBQ Beef Skewers_augmented\n",
      "✅ Lort Cha: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Lort Cha_augmented\n",
      "✅ Nom Banh Chok: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Nom Banh Chok_augmented\n",
      "✅ Omelete Rice with Pork: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Omelete Rice with Pork_augmented\n",
      "✅ Orange Fruit: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Orange Fruit_augmented\n",
      "✅ Red Curry Coconut Wings: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\Red Curry Coconut Wings_augmented\n",
      "✅ SamLar KorKour: 100 new images saved in D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\\SamLar KorKour_augmented\n",
      "🎉 Data Augmentation Completed for All Classes!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "\n",
    "# Path to the main train folder\n",
    "train_folder = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\"\n",
    "\n",
    "# Get all food class names (subdirectories in the train folder)\n",
    "food_classes = [d for d in os.listdir(train_folder) if os.path.isdir(os.path.join(train_folder, d))]\n",
    "\n",
    "# Data Augmentation Generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,       # Random rotation (0-40 degrees)\n",
    "    width_shift_range=0.2,   # Random horizontal shift\n",
    "    height_shift_range=0.2,  # Random vertical shift\n",
    "    shear_range=0.2,         # Shear transformations\n",
    "    zoom_range=0.2,          # Random zoom\n",
    "    horizontal_flip=True,    # Flip images horizontally\n",
    "    brightness_range=[0.7, 1.3],  # Vary brightness\n",
    "    fill_mode=\"nearest\"      # Fill empty pixels\n",
    ")\n",
    "\n",
    "# Number of new images per class\n",
    "num_new_images = 100\n",
    "\n",
    "# Process each food class\n",
    "for food_class in food_classes:\n",
    "    input_folder = os.path.join(train_folder, food_class)\n",
    "    output_folder = os.path.join(train_folder, f\"{food_class}_augmented\")\n",
    "\n",
    "    # Create output directory if it does not exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get list of images in the folder\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    count = 0\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(input_folder, img_file)\n",
    "        img = load_img(img_path, target_size=(224, 224))  # Resize to match model input size\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Generate new images\n",
    "        i = 0\n",
    "        for batch in datagen.flow(img_array, batch_size=1, save_to_dir=output_folder, save_prefix=\"aug\", save_format=\"jpg\"):\n",
    "            i += 1\n",
    "            count += 1\n",
    "            if count >= num_new_images:\n",
    "                break  # Stop once we reach 100 images\n",
    "        if count >= num_new_images:\n",
    "            break\n",
    "\n",
    "    print(f\"✅ {food_class}: {num_new_images} new images saved in {output_folder}\")\n",
    "\n",
    "print(\"🎉 Data Augmentation Completed for All Classes!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Amok: 100 new images added!\n",
      "✅ apple: 100 new images added!\n",
      "✅ Borbor: 100 new images added!\n",
      "✅ Cha Loc Lac: 100 new images added!\n",
      "✅ Chikhen Rice: 100 new images added!\n",
      "✅ Lemongrass BBQ Beef Skewers: 100 new images added!\n",
      "✅ Lort Cha: 100 new images added!\n",
      "✅ Nom Banh Chok: 100 new images added!\n",
      "✅ Omelete Rice with Pork: 100 new images added!\n",
      "✅ Orange Fruit: 100 new images added!\n",
      "✅ Red Curry Coconut Wings: 100 new images added!\n",
      "✅ SamLar KorKour: 100 new images added!\n",
      "🎉 Data Augmentation Completed! Now Training the Model...\n",
      "Found 1405 images belonging to 12 classes.\n",
      "Found 348 images belonging to 12 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 687ms/step - accuracy: 0.2040 - loss: 3.6503 - val_accuracy: 0.5172 - val_loss: 1.4026\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 536ms/step - accuracy: 0.6628 - loss: 1.0761 - val_accuracy: 0.8621 - val_loss: 0.5539\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 572ms/step - accuracy: 0.8906 - loss: 0.4135 - val_accuracy: 0.9454 - val_loss: 0.3438\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 539ms/step - accuracy: 0.9023 - loss: 0.3705 - val_accuracy: 0.9368 - val_loss: 0.2363\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 559ms/step - accuracy: 0.9373 - loss: 0.1891 - val_accuracy: 0.9511 - val_loss: 0.2425\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 543ms/step - accuracy: 0.9399 - loss: 0.1942 - val_accuracy: 0.9282 - val_loss: 0.2974\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 538ms/step - accuracy: 0.9469 - loss: 0.1855 - val_accuracy: 0.9684 - val_loss: 0.2075\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 565ms/step - accuracy: 0.9790 - loss: 0.0649 - val_accuracy: 0.9454 - val_loss: 0.2107\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 548ms/step - accuracy: 0.9326 - loss: 0.2409 - val_accuracy: 0.9425 - val_loss: 0.2319\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 533ms/step - accuracy: 0.9898 - loss: 0.0458 - val_accuracy: 0.9684 - val_loss: 0.1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 Model Training Completed & Saved as food_classifier.h5!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Path to the main train folder\n",
    "train_folder = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\"\n",
    "\n",
    "# Get all food class names (subdirectories in the train folder)\n",
    "food_classes = [d for d in os.listdir(train_folder) if os.path.isdir(os.path.join(train_folder, d))]\n",
    "\n",
    "# Data Augmentation Generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "# Number of new images per class\n",
    "num_new_images = 100\n",
    "\n",
    "# Apply augmentation to each food class\n",
    "for food_class in food_classes:\n",
    "    class_folder = os.path.join(train_folder, food_class)\n",
    "\n",
    "    # Get list of images in the folder\n",
    "    image_files = [f for f in os.listdir(class_folder) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    count = 0\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(class_folder, img_file)\n",
    "        img = load_img(img_path, target_size=(224, 224))  # Resize to match model input size\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "        # Generate new images and save inside the same folder\n",
    "        i = 0\n",
    "        for batch in datagen.flow(img_array, batch_size=1, save_to_dir=class_folder, save_prefix=\"aug\", save_format=\"jpg\"):\n",
    "            i += 1\n",
    "            count += 1\n",
    "            if count >= num_new_images:\n",
    "                break  # Stop once we reach 100 images\n",
    "        if count >= num_new_images:\n",
    "            break\n",
    "\n",
    "    print(f\"✅ {food_class}: {num_new_images} new images added!\")\n",
    "\n",
    "print(\"🎉 Data Augmentation Completed! Now Training the Model...\")\n",
    "\n",
    "# ---- Start Training Model ----\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_folder,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_folder,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(food_classes), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"food_classifier.h5\")\n",
    "print(\"🎉 Model Training Completed & Saved as food_classifier.h5!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Predicted Food: Nom Banh Chok\n",
      "Calories: 523.31 kcal\n",
      "Carbs: 29.95 g\n",
      "Fat: 1.73 g\n",
      "Protein: 8.11 g\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('food_classifier.h5')\n",
    "\n",
    "# Load class labels (same order as training)\n",
    "class_labels = [\n",
    "    \"Amok\", \"apple\", \"BorBor\", \"Cha Loc Lac\", \"Chikhen Rice\", \"Lemongrass BBQ Beef Skewers\", \n",
    "    \"Lort Cha\", \"Nom Banh Chok\", \"Omelete Rice with Pork\", \"Orage Fruit\", \n",
    "    \"Red Curry Coconut Wings\", \"Samlar KorKour\"\n",
    "]\n",
    "\n",
    "# Load Nutrition Data\n",
    "def load_nutrition_info(json_file=\"nutritional_data.json\"):\n",
    "    with open(json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Convert list to dictionary for easy lookup\n",
    "    return {item[\"name\"]: {\"calories\": item[\"calories\"], \"carbs\": item[\"carbs\"], \"fat\": item[\"fat\"], \"protein\": item[\"protein\"]} for item in data}\n",
    "\n",
    "nutrition_data = load_nutrition_info()\n",
    "\n",
    "# Function to preprocess and predict food from an image\n",
    "def predict_food(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Normalize\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)  # Get index of highest probability class\n",
    "    food_name = class_labels[predicted_class]\n",
    "\n",
    "    # Get nutrition info\n",
    "    nutrition_info = nutrition_data.get(food_name, {\"calories\": \"N/A\", \"carbs\": \"N/A\", \"fat\": \"N/A\", \"protein\": \"N/A\"})\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Predicted Food: {food_name}\")\n",
    "    print(f\"Calories: {nutrition_info['calories']} kcal\")\n",
    "    print(f\"Carbs: {nutrition_info['carbs']} g\")\n",
    "    print(f\"Fat: {nutrition_info['fat']} g\")\n",
    "    print(f\"Protein: {nutrition_info['protein']} g\")\n",
    "\n",
    "# Example Test\n",
    "image_path = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train/Omelete Rice with Pork/Loco_Moco_at_Ethel's_Grill,_Honolulu,_Hawaii.jpg\"  # Change this to your test image path\n",
    "\n",
    "predict_food(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset successfully split into train (70%), validation (20%), and test (10%) sets!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Original dataset folder\n",
    "dataset_path = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/train\"\n",
    "\n",
    "# New directories for split data\n",
    "train_dir = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/split/training_set\"\n",
    "val_dir = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/split/validation_set\"\n",
    "test_dir = \"D:/I4_Accadamic_Year/Semester1/AI/Group06_FruitClassification/split/testing_set\"\n",
    "\n",
    "# Create directories\n",
    "for folder in [train_dir, val_dir, test_dir]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# Loop through each food class\n",
    "for category in os.listdir(dataset_path):\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    \n",
    "    if os.path.isdir(category_path):\n",
    "        images = [f for f in os.listdir(category_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Define split sizes\n",
    "        train_split = int(0.7 * len(images))\n",
    "        val_split = int(0.2 * len(images))\n",
    "\n",
    "        # Split images\n",
    "        train_images = images[:train_split]\n",
    "        val_images = images[train_split:train_split + val_split]\n",
    "        test_images = images[train_split + val_split:]\n",
    "\n",
    "        # Create category folders in new directories\n",
    "        for target_dir in [train_dir, val_dir, test_dir]:\n",
    "            category_target = os.path.join(target_dir, category)\n",
    "            if not os.path.exists(category_target):\n",
    "                os.makedirs(category_target)\n",
    "\n",
    "        # Move images\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(category_path, img), os.path.join(train_dir, category, img))\n",
    "\n",
    "        for img in val_images:\n",
    "            shutil.copy(os.path.join(category_path, img), os.path.join(val_dir, category, img))\n",
    "\n",
    "        for img in test_images:\n",
    "            shutil.copy(os.path.join(category_path, img), os.path.join(test_dir, category, img))\n",
    "\n",
    "print(\"✅ Dataset successfully split into train (70%), validation (20%), and test (10%) sets!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1221 images belonging to 12 classes.\n",
      "Found 185 images belonging to 12 classes.\n",
      "Found 348 images belonging to 12 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 731ms/step - accuracy: 0.2304 - loss: 4.1527 - val_accuracy: 0.5081 - val_loss: 1.5272\n",
      "Epoch 2/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 575ms/step - accuracy: 0.5826 - loss: 1.2985 - val_accuracy: 0.7730 - val_loss: 0.7819\n",
      "Epoch 3/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 628ms/step - accuracy: 0.8154 - loss: 0.6077 - val_accuracy: 0.8919 - val_loss: 0.4292\n",
      "Epoch 4/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 605ms/step - accuracy: 0.9291 - loss: 0.3055 - val_accuracy: 0.8757 - val_loss: 0.4839\n",
      "Epoch 5/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 591ms/step - accuracy: 0.9273 - loss: 0.2491 - val_accuracy: 0.8919 - val_loss: 0.4303\n",
      "Epoch 6/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 564ms/step - accuracy: 0.9568 - loss: 0.1638 - val_accuracy: 0.9027 - val_loss: 0.3871\n",
      "Epoch 7/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 545ms/step - accuracy: 0.9623 - loss: 0.1098 - val_accuracy: 0.9081 - val_loss: 0.4549\n",
      "Epoch 8/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 506ms/step - accuracy: 0.9804 - loss: 0.1060 - val_accuracy: 0.8973 - val_loss: 0.6151\n",
      "Epoch 9/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 502ms/step - accuracy: 0.9747 - loss: 0.0998 - val_accuracy: 0.9081 - val_loss: 0.4444\n",
      "Epoch 10/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 498ms/step - accuracy: 0.9856 - loss: 0.0669 - val_accuracy: 0.8973 - val_loss: 0.5345\n",
      "Epoch 11/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 528ms/step - accuracy: 0.9549 - loss: 0.1751 - val_accuracy: 0.9027 - val_loss: 0.4150\n",
      "Epoch 12/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 493ms/step - accuracy: 0.9760 - loss: 0.0690 - val_accuracy: 0.9135 - val_loss: 0.3082\n",
      "Epoch 13/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 490ms/step - accuracy: 0.9777 - loss: 0.0491 - val_accuracy: 0.8919 - val_loss: 0.5777\n",
      "Epoch 14/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 497ms/step - accuracy: 0.9949 - loss: 0.0258 - val_accuracy: 0.9081 - val_loss: 0.4655\n",
      "Epoch 15/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 495ms/step - accuracy: 0.9843 - loss: 0.0614 - val_accuracy: 0.9189 - val_loss: 0.3910\n",
      "Epoch 16/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 505ms/step - accuracy: 0.9918 - loss: 0.0273 - val_accuracy: 0.9297 - val_loss: 0.3755\n",
      "Epoch 17/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 492ms/step - accuracy: 0.9972 - loss: 0.0146 - val_accuracy: 0.9405 - val_loss: 0.3028\n",
      "Epoch 18/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 495ms/step - accuracy: 0.9967 - loss: 0.0173 - val_accuracy: 0.9135 - val_loss: 0.3397\n",
      "Epoch 19/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 498ms/step - accuracy: 0.9996 - loss: 0.0068 - val_accuracy: 0.9243 - val_loss: 0.3609\n",
      "Epoch 20/20\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 491ms/step - accuracy: 0.9994 - loss: 0.0122 - val_accuracy: 0.9459 - val_loss: 0.3010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Training Completed & Saved!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Paths for split dataset\n",
    "train_path = \"C:/xampp/htdocs/php-practics/fnapp/model/split/training_set\"\n",
    "val_path = \"C:/xampp/htdocs/php-practics/fnapp/model/split/testing_set\"\n",
    "test_path = \"C:/xampp/htdocs/php-practics/fnapp/model/split/validation_set\"\n",
    "\n",
    "# Data Generators (rescaling)\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"food_classifier_model.h5\")\n",
    "\n",
    "print(\"✅ Model Training Completed & Saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Predicted Food: Amok\n",
      "Calories: 453 kcal\n",
      "Carbs: 22 g\n",
      "Fat: 9 g\n",
      "Protein: 38 g\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('food_classifier_model.h5')\n",
    "\n",
    "# Load class labels (same order as training)\n",
    "class_labels = [\n",
    "    \"Amok\", \"apple\", \"BorBor\", \"Cha Loc Lac\", \"Chikhen Rice\", \"Lemongrass BBQ Beef Skewers\", \n",
    "    \"Lort Cha\", \"Nom Banh Chok\", \"Omelete Rice with Pork\", \"Orage Fruit\", \n",
    "    \"Red Curry Coconut Wings\", \"Samlar KorKour\"\n",
    "]\n",
    "\n",
    "# Load Nutrition Data\n",
    "def load_nutrition_info(json_file=\"nutritional_data_info.json\"):\n",
    "    with open(json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Convert list to dictionary for easy lookup\n",
    "    return {item[\"name\"]: {\"calories\": item[\"calories\"], \"carbs\": item[\"carbs\"], \"fat\": item[\"fat\"], \"protein\": item[\"protein\"]} for item in data}\n",
    "\n",
    "nutrition_data = load_nutrition_info()\n",
    "\n",
    "# Function to preprocess and predict food from an image\n",
    "def predict_food(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Normalize\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)  # Get index of highest probability class\n",
    "    food_name = class_labels[predicted_class]\n",
    "\n",
    "    # Get nutrition info\n",
    "    nutrition_info = nutrition_data.get(food_name, {\"calories\": \"N/A\", \"carbs\": \"N/A\", \"fat\": \"N/A\", \"protein\": \"N/A\"})\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Predicted Food: {food_name}\")\n",
    "    print(f\"Calories: {nutrition_info['calories']} kcal\")\n",
    "    print(f\"Carbs: {nutrition_info['carbs']} g\")\n",
    "    print(f\"Fat: {nutrition_info['fat']} g\")\n",
    "    print(f\"Protein: {nutrition_info['protein']} g\")\n",
    "\n",
    "# Example Test\n",
    "image_path = \"C:/xampp/htdocs/php-practics/fnapp/model/split/testing_set/Amok/images (60).jpg\"  # Change this to your test image path\n",
    "\n",
    "predict_food(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmp3g2ednt9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmp3g2ednt9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmp3g2ednt9'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 12), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2046397345488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397342800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397344336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397341840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397336464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397343568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397334544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397346064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397342416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2046397347600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('food_classifier_model.h5')\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open('food_classifier_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
